DISCORD_BOT_TOKEN=your-discord-token
OPENAI_API_KEY=local
OPENAI_BASE_URL=http://host.docker.internal:8080/v1
OPENAI_SUMMARY_MODEL=/path/to/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf
CACHE_DB=/app/data/bot_cache.sqlite3
MAX_CHARS_PER_CHUNK=12000
MAX_DISCORD_MSG_CHARS=1900

# Model context window (match your llama-server --ctx-size)
CONTEXT_TOKENS=4096

# Token budget allocation
PROMPT_TOKENS=500
OUTPUT_TOKENS=600
SAFETY_TOKENS=200
SUMMARY_MAX_TOKENS=600

# Concurrency and timeouts
LLM_CONCURRENCY=4
PER_CALL_TIMEOUT_SEC=300
MAX_TOTAL_RUNTIME_SEC=36000

# Processing limits
MAX_PROCESSING_SECONDS=600
TOKENS_PER_SECOND=20
API_OVERHEAD_SEC=2.0

# Optional: YouTube API hardening
# YT_COOKIES=/path/to/cookies.txt
# YT_FORCE_IPV4=1
# YT_REQ_SLEEP=0.5

# Optional: Rate limiting and quotas
# RATE_RPS=1.0
# RATE_BURST=2
# USER_QUOTA_MAX=5
# USER_QUOTA_WINDOW=600
# CHAN_QUOTA_MAX=20
# CHAN_QUOTA_WINDOW=600
# CB_429_THRESHOLD=3
# CB_OPEN_SECS=1800
# CACHE_DB_PATH=/app/cache.db
